import json
import os
import random
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from collections import defaultdict
from tqdm import tqdm
import traceback
import networkx as nx
import pickle
import data_generate
import fsp_generate
from fsp_generate.code.build_dependency_graph.prompt.tool_file_dependency_prompt import tool_file_dependency_prompt
from data_generate.agent.model.gpt import ChatGPTFunction
from data_generate.agent.model.qwq import QwQFunction
from fsp_generate.utils.load_utils import *

        
def process_one_dependency(llm, file_path, file_info, batch_size=5):
    
    # è·å–å€™é€‰å‡½æ•°ï¼ˆåŒç±»ä¸­é™¤è‡ªå·±å¤–ï¼‰
    candidates = [func for func in function_pool_index]
    uncached = [c for c in candidates if (c, file_path) not in relation_cache]

    # å…ˆæŠŠç¼“å­˜ä¸­å·²æœ‰çš„ä¿¡æ¯æ·»åŠ è¿›å›¾
    for c in candidates:
        if (c, file_path) in relation_cache and relation_cache[(c, file_path)]:
            G.add_edge(c, file_path)
            # print(f"âœ… è½½å…¥å·²æœ‰ä¾èµ–: {c} -> {file_path}")

    # å¦‚æœå…¨éƒ¨éƒ½æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if not uncached:
        return

    print(f"â¡ï¸ æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
    # æŒ‰ batch åˆ†æ‰¹å¤„ç†æœªç¼“å­˜çš„ candidate
    for i in range(0, len(uncached), batch_size):
        batch = uncached[i:i + batch_size]
        sampled_infos = [function_pool_index[c] for c in batch]

        prompt_parameter = {
            "candidate_functions": sampled_infos,
            "file_info": file_info,
        }

        for attempt in range(5):
            try:
                response,status_code = llm_request(llm, tool_file_dependency_prompt, prompt_parameter)
                if status_code!=200:
                    raise Exception(f'Error with code {status_code}: {response}')
                response=response['content']
                try:
                    if '```json' in response:
                        matches = re.findall(r'```json(.*?)```', response, re.DOTALL)
                        response = json.loads(matches[0].strip())
                    elif isinstance(response, str):
                        response = json.loads(response.strip())
                    use_files = response["functions_using_file"]
                except:
                    raise Exception(f'cannot extract: {response}')
                
                if not use_files:
                    print(f"âŒ æ— ä¾èµ–: {file_path} -> {c}")
                else:
                    print(use_files)
                    print(response.get("reason", ""))
                
                with lock:
                    for c in batch:
                        relation_cache[(c, file_path)] = c in use_files
                        if c in use_files:
                            G.add_edge(c, file_path)
                            print(f"âœ… æ·»åŠ ä¾èµ–: {c} -> {file_path}")

                    # ğŸ§  æ¯ batch åä¿å­˜ï¼ˆå¯é€‰ï¼šåˆ¤æ–­æ˜¯å¦æœ‰æ–°å†…å®¹å†ä¿å­˜ï¼‰
                    save_relation_cache(relation_cache, relation_cache_file)

                # print(f"âœ… Batch å¤„ç†å®Œæˆ: {func_name} -> {batch}")
                break  # æˆåŠŸåè·³å‡º retry å¾ªç¯
            except Exception as e:
                print(f"[ERROR][{file_path}] Attempt {attempt + 1} failed: {e}")

def main(llm_name='gpt',thread_num=10):
    if llm_name=='gpt':
        llm = ChatGPTFunction(name='gpt4o-ptu-client')
    else:
        llm = QwQFunction()
    tasks = []
    with ThreadPoolExecutor(max_workers=thread_num) as executor:
        for file_path, file_info in file_pool_index.items():
            tasks.append(executor.submit(process_one_dependency, llm, file_path, file_info))
        for future in tqdm(as_completed(tasks), total=len(tasks), desc="â³ ç­‰å¾…çº¿ç¨‹å®Œæˆ"):
            try:
                future.result(timeout=60) 
            except Exception as e:
                print(f"[Thread Error] {e}")
                traceback.print_exc()

    save_relation_cache(relation_cache, relation_cache_file)
    print("âœ… æ‰€æœ‰ä¾èµ–å…³ç³»æ„å»ºå®Œæˆã€‚")


if __name__ == "__main__":
    import data_generate
    fsp_dir = os.path.dirname(fsp_generate.__file__)
    project_dir = os.path.dirname(data_generate.__file__)
    tool_metadata_path = f"{project_dir}/tools/all_tools_metadata.json"
    file_metadata_path = f"{project_dir}/working_dir/all_files_metadata.json"
    save_dir = f"{fsp_dir}/data/graph"
    os.makedirs(save_dir, exist_ok=True)

    function_pool_index = load_functions(tool_metadata_path,limit_categories=['file_system_functions', 'python_functions', 'database_functions','api_functions'])
    file_pool_index = load_file_pools(file_metadata_path)
    all_results = []
    lock = Lock()
    G = nx.DiGraph()

    relation_cache_file = os.path.join(save_dir, "file_tool_dependency_graph.json")
    relation_cache = load_relation_cache(relation_cache_file)

    main(llm_name='gpt',thread_num=10)
