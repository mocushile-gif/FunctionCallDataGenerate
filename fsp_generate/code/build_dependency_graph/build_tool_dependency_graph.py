import json
import os
import random
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from collections import defaultdict
from tqdm import tqdm
import networkx as nx
import pickle
import fsp_generate
from fsp_generate.code.build_dependency_graph.prompt.tool_dependency_prompt import tool_dependency_prompt
from data_generate.agent.model.gpt import ChatGPTFunction
from data_generate.agent.model.qwq import QwQFunction
from fsp_generate.utils.load_utils import *

def load_function_tools(tool_metadata_path):
    temp_function_pool_index = defaultdict(lambda: defaultdict(dict))
    print("ğŸ” åŠ è½½å‡½æ•°æ± ...")
    with open(self_tool_res_cache_path, 'r') as f:
        self_tool_cache = json.load(f)
    with open(rapidapi_res_cache_path, 'r') as f:
        rapidapi_cache = json.load(f)
    with open(tool_metadata_path, "r") as f:
        func_data = json.load(f)
        for function in func_data.values():
            define = function["api_define"]
            cache_responses = self_tool_cache.get(define["name"], {}) if function["category"] in [
                'file_system_functions', 'python_functions', 'database_functions', 'api_functions'
            ] else rapidapi_cache.get(define["name"], {})

            example_num = 5
            example_responses = list(cache_responses.items())[:example_num]
            while len(str(example_responses)) > 4096 and example_num > 1:
                example_num -= 1
                example_responses = list(cache_responses.items())[:example_num]
            example_responses = [{"input":example[0],"response":example[1]} for example in example_responses]
            
            func_info = {"define": define, "example_responses": example_responses}
            category = function["category"] if function["category"] not in [
                'file_system_functions', 'python_functions', 'database_functions','api_functions'
            ] else 'self-tools'
            temp_function_pool_index[category][define["name"]] = func_info

    for category, values in temp_function_pool_index.items():
        if len(values) < 30:
            function_pool_index['Others'].update(values)
        else:
            function_pool_index[category] = values
    for category in function_pool_index:
        print(f"âœ… åŠ è½½ {category} å‡½æ•°æ•°é‡: {len(function_pool_index[category])}")


def process_one_dependency(llm, category, func_name, func_info, batch_size=10):
    
    # è·å–å€™é€‰å‡½æ•°ï¼ˆåŒç±»ä¸­é™¤è‡ªå·±å¤–ï¼‰
    # candidates = [func for func in function_pool_index[category] if func != func_name]
    # uncached = [c for c in candidates if (func_name, c) not in relation_cache]
    category_funcs = list(function_pool_index[category].keys())
    candidates = [f for f in category_funcs if f != func_name]
    seen = relation_index.get(func_name, set())
    uncached = [c for c in candidates if c not in seen]
    # å…ˆæŠŠç¼“å­˜ä¸­å·²æœ‰çš„ä¿¡æ¯æ·»åŠ è¿›å›¾
    # for c in candidates:
    #     if (func_name, c) in relation_cache and relation_cache[(func_name, c)]:
    #         G.add_edge(func_name, c)
            # print(f"âœ… è½½å…¥å·²æœ‰ä¾èµ–: {func_name} -> {c}")

    # å¦‚æœå…¨éƒ¨éƒ½æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if not uncached:
        # print(f'âœ… [{category}] {func_name}å¤„ç†å®Œæ¯•')
        return
    # print(f'âœ… [{category}] {func_name}ä»éœ€å¤„ç†ï¼š{uncached}')
    print(f'â¡ï¸ [{category}] {func_name}ä»éœ€å¤„ç†ï¼š{len(uncached)}/{len(candidates)}')
    # print(f"â¡ï¸  [{category}] æ­£åœ¨å¤„ç†å‡½æ•°: {func_name}")
    # æŒ‰ batch åˆ†æ‰¹å¤„ç†æœªç¼“å­˜çš„ candidate
    for i in range(0, len(uncached), batch_size):
        batch = uncached[i:i + batch_size]
        sampled_infos = [function_pool_index[category][c] for c in batch]

        prompt_parameter = {
            "candidate_functions": sampled_infos,
            "source_function": func_info["define"],
            "source_function_output": func_info["example_responses"]
        }

        for attempt in range(3):
            try:
                response,status_code = llm_request(llm, tool_dependency_prompt, prompt_parameter)
                # print(response)
                if status_code!=200:
                    raise Exception(f'Error with code {status_code}: {response}')
                response=response['content']
                try:
                    if '```json' in response:
                        matches = re.findall(r'```json(.*?)```', response, re.DOTALL)
                        response = json.loads(matches[0].strip())
                    else:
                        response = json.loads(response.strip())
                    
                    dependent_funcs = response["dependent_functions"]
                except:
                    raise Exception(f'cannot extract: {response}')

                if not dependent_funcs:
                    print(f"âŒ æ— ä¾èµ–: {func_name} -> {batch}")
                else:
                    print(dependent_funcs)
                    print(response.get("reason", ""))

                with lock:
                    for c in batch:
                        relation_cache[(func_name, c)] = c in dependent_funcs
                        relation_index[func_name].add(c)
                        if c in dependent_funcs:
                        #     G.add_edge(func_name, c)
                            print(f"âœ… æ·»åŠ ä¾èµ–: {func_name} -> {c}")

                    # ğŸ§  æ¯ batch åä¿å­˜ï¼ˆå¯é€‰ï¼šåˆ¤æ–­æ˜¯å¦æœ‰æ–°å†…å®¹å†ä¿å­˜ï¼‰

                    save_relation_cache(relation_cache, relation_cache_file)
                # print(f"âœ… Batch å¤„ç†å®Œæˆ: {func_name} -> {batch}")
                break  # æˆåŠŸåè·³å‡º retry å¾ªç¯
            except Exception as e:
                print(f"[ERROR][{category}::{func_name}] Attempt {attempt + 1} failed: {e}")
            assert len(relation_index[func_name])==len(candidates)


def main(llm_name='gpt',thread_num=10):
    if llm_name=='gpt':
        llm = ChatGPTFunction(name='gpt4o-ptu-client')
    else:
        llm = QwQFunction()
    tasks = []
    with ThreadPoolExecutor(max_workers=thread_num) as executor:
        for category, functions_group in function_pool_index.items():
            for current_func, function_info in tqdm(functions_group.items(), desc=f"ğŸ“‚  â†’ {category}", leave=False):
                tasks.append(executor.submit(process_one_dependency, llm, category, current_func, function_info))
        for future in tqdm(as_completed(tasks), total=len(tasks), desc="â³ ç­‰å¾…çº¿ç¨‹å®Œæˆ"):
            try:
                future.result()
            except Exception as e:
                print(f"[Thread Error] {e}")

    save_relation_cache(relation_cache, relation_cache_file)
    print("âœ… æ‰€æœ‰ä¾èµ–å…³ç³»æ„å»ºå®Œæˆ")

if __name__ == "__main__":
    import data_generate
    fsp_dir = os.path.dirname(fsp_generate.__file__)
    project_dir = os.path.dirname(data_generate.__file__)
    self_tool_res_cache_path = f'{fsp_dir}/tools/tool_response_collections_self.json'
    rapidapi_res_cache_path = f'{fsp_dir}/tools/tool_response_collections_rapidapi.json'
    tool_metadata_path = f"{project_dir}/tools/all_tools_metadata.json"
    save_dir = f"{fsp_dir}/data/graph"
    os.makedirs(save_dir, exist_ok=True)

    function_pool_index = defaultdict(lambda: defaultdict(dict))
    load_function_tools(tool_metadata_path)
    # G = nx.DiGraph()
    all_results = []
    lock = Lock()

    relation_cache_file = os.path.join(save_dir, "tool_dependency_graph.json")
    relation_cache = load_relation_cache(relation_cache_file)
    relation_index = defaultdict(set)
    for (src, tgt) in relation_cache.keys():
        relation_index[src].add(tgt)
    main(llm_name='gpt',thread_num=10)
